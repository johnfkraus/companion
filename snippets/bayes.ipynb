{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/\n",
    "Machine Learning Mastery\n",
    "\n",
    "How to Develop a Naive Bayes Classifier from Scratch in Python\n",
    "by Jason Brownlee on October 7, 2019 in Probability\n",
    "Last Updated on January 10, 2020\n",
    "\n",
    "Classification is a predictive modeling problem that involves assigning a label to a given input data sample.\n",
    "\n",
    "The problem of classification predictive modeling can be framed as calculating the conditional probability of a class label given a data sample. Bayes Theorem provides a principled way for calculating this conditional probability, although in practice requires an enormous number of samples (very large-sized dataset) and is computationally expensive.\n",
    "\n",
    "Instead, the calculation of Bayes Theorem can be simplified by making some assumptions, such as each input variable is independent of all other input variables. Although a dramatic and unrealistic assumption, this has the effect of making the calculations of the conditional probability tractable and results in an effective classification model referred to as Naive Bayes.\n",
    "\n",
    "In this tutorial, you will discover the Naive Bayes algorithm for classification predictive modeling.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "How to frame classification predictive modeling as a conditional probability model.\n",
    "How to use Bayes Theorem to solve the conditional probability model of classification.\n",
    "How to implement simplified Bayes Theorem for classification, called the Naive Bayes algorithm.\n",
    "Kick-start your project with my new book Probability for Machine Learning, including step-by-step tutorials and the Python source code files for all examples.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "Tutorial Overview\n",
    "This tutorial is divided into five parts; they are:\n",
    "\n",
    "Conditional Probability Model of Classification\n",
    "Simplified or Naive Bayes\n",
    "How to Calculate the Prior and Conditional Probabilities\n",
    "Worked Example of Naive Bayes\n",
    "5 Tips When Using Naive Bayes\n",
    "\n",
    "Conditional Probability Model of Classification\n",
    "\n",
    "In machine learning, we are often interested in a predictive modeling problem where we want to predict a class label for a given observation. For example, classifying the species of plant based on measurements of the flower.\n",
    "\n",
    "Problems of this type are referred to as classification predictive modeling problems, as opposed to regression problems that involve predicting a numerical value. The observation or input to the model is referred to as X and the class label or output of the model is referred to as y.\n",
    "\n",
    "Together, X and y represent observations collected from the domain, i.e. a table or matrix (columns and rows or features and samples) of training data used to fit a model. The model must learn how to map specific examples to class labels or y = f(X) that minimized the error of misclassification.\n",
    "\n",
    "One approach to solving this problem is to develop a probabilistic model. From a probabilistic perspective, we are interested in estimating the conditional probability of the class label, given the observation.\n",
    "\n",
    "For example, a classification problem may have k class labels y1, y2, …, yk and n input variables, X1, X2, …, Xn. We can calculate the conditional probability for a class label with a given instance or set of input values for each column x1, x2, …, xn as follows:\n",
    "\n",
    "P(yi | x1, x2, …, xn)\n",
    "The conditional probability can then be calculated for each class label in the problem and the label with the highest probability can be returned as the most likely classification.\n",
    "\n",
    "The conditional probability can be calculated using the joint probability, although it would be intractable. Bayes Theorem provides a principled way for calculating the conditional probability.\n",
    "\n",
    "The simple form of the calculation for Bayes Theorem is as follows:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "Where the probability that we are interested in calculating P(A|B) is called the posterior probability and the marginal probability of the event P(A) is called the prior.\n",
    "\n",
    "We can frame classification as a conditional classification problem with Bayes Theorem as follows:\n",
    "\n",
    "P(yi | x1, x2, …, xn) = P(x1, x2, …, xn | yi) * P(yi) / P(x1, x2, …, xn)\n",
    "\n",
    "The prior P(yi) is easy to estimate from a dataset, but the conditional probability of the observation based on the class P(x1, x2, …, xn | yi) is not feasible unless the number of examples is extraordinarily large, e.g. large enough to effectively estimate the probability distribution for all different possible combinations of values.\n",
    "\n",
    "As such, the direct application of Bayes Theorem also becomes intractable, especially as the number of variables or features (n) increases.\n",
    "\n",
    "Simplified or Naive Bayes\n",
    "\n",
    "The solution to using Bayes Theorem for a conditional probability classification model is to simplify the calculation.\n",
    "\n",
    "The Bayes Theorem assumes that each input variable is dependent upon all other variables. This is a cause of complexity in the calculation. We can remove this assumption and consider each input variable as being independent from each other.\n",
    "\n",
    "This changes the model from a dependent conditional probability model to an independent conditional probability model and dramatically simplifies the calculation.\n",
    "\n",
    "First, the denominator is removed from the calculation P(x1, x2, …, xn) as it is a constant used in calculating the conditional probability of each class for a given instance and has the effect of normalizing the result.\n",
    "\n",
    "P(yi | x1, x2, …, xn) = P(x1, x2, …, xn | yi) * P(yi)\n",
    "\n",
    "Next, the conditional probability of all variables given the class label is changed into separate conditional probabilities of each variable value given the class label. These independent conditional variables are then multiplied together. For example:\n",
    "\n",
    "P(yi | x1, x2, …, xn) = P(x1|yi) * P(x2|yi) * … P(xn|yi) * P(yi)\n",
    "\n",
    "This calculation can be performed for each of the class labels, and the label with the largest probability can be selected as the classification for the given instance. This decision rule is referred to as the maximum a posteriori, or MAP, decision rule.\n",
    "\n",
    "This simplification of Bayes Theorem is common and widely used for classification predictive modeling problems and is generally referred to as Naive Bayes.\n",
    "\n",
    "The word “naive” is French and typically has a diaeresis (umlaut) over the “i”, which is commonly left out for simplicity, and “Bayes” is capitalized as it is named for Reverend Thomas Bayes.\n",
    "\n",
    "How to Calculate the Prior and Conditional Probabilities\n",
    "\n",
    "Now that we know what Naive Bayes is, we can take a closer look at how to calculate the elements of the equation.\n",
    "\n",
    "The calculation of the prior P(yi) is straightforward. It can be estimated by dividing the frequency of observations in the training dataset that have the class label by the total number of examples (rows) in the training dataset. For example:\n",
    "\n",
    "P(yi) = examples with yi / total examples\n",
    "\n",
    "The conditional probability for a feature value given the class label can also be estimated from the data. Specifically, those data examples that belong to a given class, and one data distribution per variable. This means that if there are K classes and n variables, that k * n different probability distributions must be created and maintained.\n",
    "\n",
    "A different approach is required depending on the data type of each feature. Specifically, the data is used to estimate the parameters of one of three standard probability distributions.\n",
    "\n",
    "In the case of categorical variables, such as counts or labels, a multinomial distribution can be used. If the variables are binary, such as yes/no or true/false, a binomial distribution can be used. If a variable is numerical, such as a measurement, often a Gaussian distribution is used.\n",
    "\n",
    "Binary: Binomial distribution.\n",
    "Categorical: Multinomial distribution.\n",
    "Numeric: Gaussian distribution.\n",
    "\n",
    "These three distributions are so common that the Naive Bayes implementation is often named after the distribution. For example:\n",
    "\n",
    "Binomial Naive Bayes: Naive Bayes that uses a binomial distribution.\n",
    "Multinomial Naive Bayes: Naive Bayes that uses a multinomial distribution.\n",
    "Gaussian Naive Bayes: Naive Bayes that uses a Gaussian distribution.\n",
    "\n",
    "A dataset with mixed data types for the input variables may require the selection of different types of data distributions for each variable.\n",
    "\n",
    "Using one of the three common distributions is not mandatory; for example, if a real-valued variable is known to have a different specific distribution, such as exponential, then that specific distribution may be used instead. If a real-valued variable does not have a well-defined distribution, such as bimodal or multimodal, then a kernel density estimator can be used to estimate the probability distribution instead.\n",
    "\n",
    "The Naive Bayes algorithm has proven effective and therefore is popular for text classification tasks. The words in a document may be encoded as binary (word present), count (word occurrence), or frequency (tf/idf) input vectors and binary, multinomial, or Gaussian probability distributions used respectively.\n",
    "\n",
    "Worked Example of Naive Bayes\n",
    "\n",
    "In this section, we will make the Naive Bayes calculation concrete with a small example on a machine learning dataset.\n",
    "\n",
    "We can generate a small contrived binary (2 class) classification problem using the make_blobs() function from the scikit-learn API.\n",
    "\n",
    "The example below generates 100 examples with two numerical input variables, each assigned one of two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n",
      "[[-0.79415228  2.10495117]\n",
      " [-9.15155186 -4.81286449]\n",
      " [-3.10367371  3.90202401]\n",
      " [-1.42946517  5.16850105]\n",
      " [-7.4693868  -4.20198333]]\n",
      "[0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example of generating a small classification dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# summarize\n",
    "print(X.shape, y.shape)\n",
    "print(X[:5])\n",
    "print(y[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example generates the dataset and summarizes the size, confirming the dataset was generated as expected.\n",
    "\n",
    "The “random_state” argument is set to 1, ensuring that the same random sample of observations is generated each time the code is run.\n",
    "\n",
    "The input and output elements of the first five examples are also printed, showing that indeed, the two input variables are numeric and the class labels are either 0 or 1 for each example.\n",
    "\n",
    "We will model the numerical input variables using a Gaussian probability distribution.\n",
    "\n",
    "This can be achieved using the norm SciPy API. First, the distribution can be constructed by specifying the parameters of the distribution, e.g. the mean and standard deviation, then the probability density function can be sampled for specific values using the norm.pdf() function.\n",
    "\n",
    "We can estimate the parameters of the distribution from the dataset using the mean() and std() NumPy functions.\n",
    "\n",
    "The fit_distribution() function below takes a sample of data for one variable and fits a data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a probability distribution to a univariate data sample\n",
    "def fit_distribution(data):\n",
    "\t# estimate parameters\n",
    "\tmu = mean(data)\n",
    "\tsigma = std(data)\n",
    "\tprint(mu, sigma)\n",
    "\t# fit distribution\n",
    "\tdist = norm(mu, sigma)\n",
    "\treturn dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are interested in the conditional probability of each input variable. This means we need one distribution for each of the input variables, and one set of distributions for each of the class labels, or four distributions in total.\n",
    "\n",
    "First, we must split the data into groups of samples for each of the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2) (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# sort data into classes\n",
    "Xy0 = X[y == 0]\n",
    "Xy1 = X[y == 1]\n",
    "print(Xy0.shape, Xy1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use these groups to calculate the prior probabilities for a data sample belonging to each group.\n",
    "\n",
    "This will be 50% exactly given that we have created the same number of examples in each of the two classes; nevertheless, we will calculate these priors for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "# calculate priors\n",
    "priory0 = len(Xy0) / len(X)\n",
    "priory1 = len(Xy1) / len(X)\n",
    "print(priory0, priory1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we can call the fit_distribution() function that we defined to prepare a probability distribution for each variable, for each class label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PDFs for y==0\n",
    "X1y0 = fit_distribution(Xy0[:, 0])\n",
    "X2y0 = fit_distribution(Xy0[:, 1])\n",
    "# create PDFs for y==1\n",
    "X1y1 = fit_distribution(Xy1[:, 0])\n",
    "X2y1 = fit_distribution(Xy1[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tying this all together, the complete probabilistic model of the dataset is listed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2) (50, 2)\n",
      "0.5 0.5\n",
      "-1.5632888906409914 0.787444265443213\n",
      "4.426680361487157 0.958296071258367\n",
      "-9.681177100524485 0.8943078901048118\n",
      "-3.9713794295185845 0.9308177595208521\n"
     ]
    }
   ],
   "source": [
    "# summarize probability distributions of the dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import norm\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    " \n",
    "# fit a probability distribution to a univariate data sample\n",
    "def fit_distribution(data):\n",
    "\t# estimate parameters\n",
    "\tmu = mean(data)\n",
    "\tsigma = std(data)\n",
    "\tprint(mu, sigma)\n",
    "\t# fit distribution\n",
    "\tdist = norm(mu, sigma)\n",
    "\treturn dist\n",
    " \n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# sort data into classes\n",
    "Xy0 = X[y == 0]\n",
    "Xy1 = X[y == 1]\n",
    "print(Xy0.shape, Xy1.shape)\n",
    "# calculate priors\n",
    "priory0 = len(Xy0) / len(X)\n",
    "priory1 = len(Xy1) / len(X)\n",
    "print(priory0, priory1)\n",
    "# create PDFs for y==0\n",
    "X1y0 = fit_distribution(Xy0[:, 0])\n",
    "X2y0 = fit_distribution(Xy0[:, 1])\n",
    "# create PDFs for y==1\n",
    "X1y1 = fit_distribution(Xy1[:, 0])\n",
    "X2y1 = fit_distribution(Xy1[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Running the example first splits the dataset into two groups for the two class labels and confirms the size of each group is even and the priors are 50%.\n",
    "\n",
    "Probability distributions are then prepared for each variable for each class label and the mean and standard deviation parameters of each distribution are reported, confirming that the distributions differ.\n",
    "\n",
    "See output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the prepared probabilistic model to make a prediction.\n",
    "\n",
    "The independent conditional probability for each class label can be calculated using the prior for the class (50%) and the conditional probability of the value for each variable.\n",
    "\n",
    "The probability() function below performs this calculation for one input example (array of two values) given the prior and conditional probability distribution for each variable. The value returned is a score rather than a probability as the quantity is not normalized, a simplification often performed when implementing naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the independent conditional probability\n",
    "def probability(X, prior, dist1, dist2):\n",
    "\treturn prior * dist1.pdf(X[0]) * dist2.pdf(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to calculate the probability for an example belonging to each class.\n",
    "\n",
    "First, we can select an example to be classified; in this case, the first example in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify one example\n",
    "Xsample, ysample = X[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we can calculate the score of the example belonging to the first class, then the second class, then report the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'priory0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/john.kraus/workspaces/mysnorkel/practical-weak-supervision/snippets/bayes.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/john.kraus/workspaces/mysnorkel/practical-weak-supervision/snippets/bayes.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m py0 \u001b[39m=\u001b[39m probability(Xsample, priory0, distX1y0, distX2y0)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/john.kraus/workspaces/mysnorkel/practical-weak-supervision/snippets/bayes.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m py1 \u001b[39m=\u001b[39m probability(Xsample, priory1, distX1y1, distX2y1)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/john.kraus/workspaces/mysnorkel/practical-weak-supervision/snippets/bayes.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mP(y=0 | \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) = \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (Xsample, py0\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'priory0' is not defined"
     ]
    }
   ],
   "source": [
    "py0 = probability(Xsample, priory0, distX1y0, distX2y0)\n",
    "py1 = probability(Xsample, priory1, distX1y1, distX2y1)\n",
    "print('P(y=0 | %s) = %.3f' % (Xsample, py0*100))\n",
    "print('P(y=1 | %s) = %.3f' % (Xsample, py1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The class with the largest score will be the resulting classification.\n",
    "\n",
    "Tying this together, the complete example of fitting the Naive Bayes model and using it to make one prediction is listed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example of preparing and making a prediction with a naive bayes model\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import norm\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    " \n",
    "# fit a probability distribution to a univariate data sample\n",
    "def fit_distribution(data):\n",
    "\t# estimate parameters\n",
    "\tmu = mean(data)\n",
    "\tsigma = std(data)\n",
    "\tprint(mu, sigma)\n",
    "\t# fit distribution\n",
    "\tdist = norm(mu, sigma)\n",
    "\treturn dist\n",
    " \n",
    "# calculate the independent conditional probability\n",
    "def probability(X, prior, dist1, dist2):\n",
    "\treturn prior * dist1.pdf(X[0]) * dist2.pdf(X[1])\n",
    " \n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# sort data into classes\n",
    "Xy0 = X[y == 0]\n",
    "Xy1 = X[y == 1]\n",
    "# calculate priors\n",
    "priory0 = len(Xy0) / len(X)\n",
    "priory1 = len(Xy1) / len(X)\n",
    "# create PDFs for y==0\n",
    "distX1y0 = fit_distribution(Xy0[:, 0])\n",
    "distX2y0 = fit_distribution(Xy0[:, 1])\n",
    "# create PDFs for y==1\n",
    "distX1y1 = fit_distribution(Xy1[:, 0])\n",
    "distX2y1 = fit_distribution(Xy1[:, 1])\n",
    "# classify one example\n",
    "Xsample, ysample = X[0], y[0]\n",
    "py0 = probability(Xsample, priory0, distX1y0, distX2y0)\n",
    "py1 = probability(Xsample, priory1, distX1y1, distX2y1)\n",
    "print('P(y=0 | %s) = %.3f' % (Xsample, py0*100))\n",
    "print('P(y=1 | %s) = %.3f' % (Xsample, py1*100))\n",
    "print('Truth: y=%d' % ysample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Running the example first prepares the prior and conditional probabilities as before, then uses them to make a prediction for one example.\n",
    "\n",
    "The score of the example belonging to y=0 is about 0.3 (recall this is an unnormalized probability), whereas the score of the example belonging to y=1 is 0.0. Therefore, we would classify the example as belonging to y=0.\n",
    "\n",
    "In this case, the true or actual outcome is known, y=0, which matches the prediction by our Naive Bayes model.\n",
    "\n",
    "P(y=0 | [-0.79415228  2.10495117]) = 0.348\n",
    "P(y=1 | [-0.79415228  2.10495117]) = 0.000\n",
    "Truth: y=0\n",
    "In practice, it is a good idea to use optimized implementations of the Naive Bayes algorithm. The scikit-learn library provides three implementations, one for each of the three main probability distributions; for example, BernoulliNB, MultinomialNB, and GaussianNB for binomial, multinomial and Gaussian distributed input variables respectively.\n",
    "\n",
    "To use a scikit-learn Naive Bayes model, first the model is defined, then it is fit on the training dataset. Once fit, probabilities can be predicted via the predict_proba() function and class labels can be predicted directly via the predict() function.\n",
    "\n",
    "The complete example of fitting a Gaussian Naive Bayes model (GaussianNB) to the same test dataset is listed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example of gaussian naive bayes\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# define the model\n",
    "model = GaussianNB()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# select a single sample\n",
    "Xsample, ysample = [X[0]], y[0]\n",
    "# make a probabilistic prediction\n",
    "yhat_prob = model.predict_proba(Xsample)\n",
    "print('Predicted Probabilities: ', yhat_prob)\n",
    "# make a classification prediction\n",
    "yhat_class = model.predict(Xsample)\n",
    "print('Predicted Class: ', yhat_class)\n",
    "print('Truth: y=%d' % ysample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Running the example fits the model on the training dataset, then makes predictions for the same first example that we used in the prior example.\n",
    "\n",
    "In this case, the probability of the example belonging to y=0 is 1.0 or a certainty. The probability of y=1 is a very small value close to 0.0.\n",
    "\n",
    "Finally, the class label is predicted directly, again matching the ground truth for the example.\n",
    "\n",
    "Predicted Probabilities:  [[1.00000000e+00 5.52387327e-30]]\n",
    "Predicted Class:  [0]\n",
    "Truth: y=0\n",
    "5 Tips When Using Naive Bayes\n",
    "This section lists some practical tips when working with Naive Bayes models.\n",
    "\n",
    "1. Use a KDE for Complex Distributions\n",
    "If the probability distribution for a variable is complex or unknown, it can be a good idea to use a kernel density estimator or KDE to approximate the distribution directly from the data samples.\n",
    "\n",
    "A good example would be the Gaussian KDE.\n",
    "\n",
    "2. Decreased Performance With Increasing Variable Dependence\n",
    "By definition, Naive Bayes assumes the input variables are independent of each other.\n",
    "\n",
    "This works well most of the time, even when some or most of the variables are in fact dependent. Nevertheless, the performance of the algorithm degrades the more dependent the input variables happen to be.\n",
    "\n",
    "3. Avoid Numerical Underflow with Log\n",
    "The calculation of the independent conditional probability for one example for one class label involves multiplying many probabilities together, one for the class and one for each input variable. As such, the multiplication of many small numbers together can become numerically unstable, especially as the number of input variables increases.\n",
    "\n",
    "To overcome this problem, it is common to change the calculation from the product of probabilities to the sum of log probabilities. For example:\n",
    "\n",
    "P(yi | x1, x2, …, xn) = log(P(x1|y1)) + log(P(x2|y1)) + … log(P(xn|y1)) + log(P(yi))\n",
    "Calculating the natural logarithm of probabilities has the effect of creating larger (negative) numbers and adding the numbers together will mean that larger probabilities will be closer to zero. The resulting values can still be compared and maximized to give the most likely class label.\n",
    "\n",
    "This is often called the log-trick when multiplying probabilities.\n",
    "\n",
    "4. Update Probability Distributions\n",
    "As new data becomes available, it can be relatively straightforward to use this new data with the old data to update the estimates of the parameters for each variable’s probability distribution.\n",
    "\n",
    "This allows the model to easily make use of new data or the changing distributions of data over time.\n",
    "\n",
    "5. Use as a Generative Model\n",
    "The probability distributions will summarize the conditional probability of each input variable value for each class label.\n",
    "\n",
    "These probability distributions can be useful more generally beyond use in a classification model.\n",
    "\n",
    "For example, the prepared probability distributions can be randomly sampled in order to create new plausible data instances. The conditional independence assumption assumed may mean that the examples are more or less plausible based on how much actual interdependence exists between the input variables in the dataset.\n",
    "\n",
    "Further Reading\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "\n",
    "Tutorials\n",
    "Naive Bayes Tutorial for Machine Learning\n",
    "Naive Bayes for Machine Learning\n",
    "Better Naive Bayes: 12 Tips To Get The Most From The Naive Bayes Algorithm\n",
    "Books\n",
    "Machine Learning, 1997.\n",
    "Machine Learning: A Probabilistic Perspective, 2012.\n",
    "Pattern Recognition and Machine Learning, 2006.\n",
    "Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.\n",
    "API\n",
    "sklearn.datasets.make_blobs API.\n",
    "scipy.stats.norm API.\n",
    "Naive Bayes, scikit-learn documentation.\n",
    "sklearn.naive_bayes.GaussianNB API\n",
    "Articles\n",
    "Bayes’ theorem, Wikipedia.\n",
    "Naive Bayes classifier, Wikipedia.\n",
    "Maximum a posteriori estimation, Wikipedia.\n",
    "Summary\n",
    "In this tutorial, you discovered the Naive Bayes algorithm for classification predictive modeling.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "How to frame classification predictive modeling as a conditional probability model.\n",
    "How to use Bayes Theorem to solve the conditional probability model of classification.\n",
    "How to implement simplified Bayes Theorem for classification called the Naive Bayes algorithm.\n",
    "Do you have any questions?\n",
    "Ask your questions in the comments below and I will do my best to answer.\n",
    "\n",
    "Get a Handle on Probability for Machine Learning!\n",
    "Probability for Machine Learning\n",
    "Develop Your Understanding of Probability\n",
    "...with just a few lines of python code\n",
    "Discover how in my new Ebook:\n",
    "Probability for Machine Learning\n",
    "\n",
    "It provides self-study tutorials and end-to-end projects on:\n",
    "Bayes Theorem, Bayesian Optimization, Distributions, Maximum Likelihood, Cross-Entropy, Calibrating Models\n",
    "and much more...\n",
    "\n",
    "Finally Harness Uncertainty in Your Projects\n",
    "Skip the Academics. Just Results.\n",
    "SEE WHAT'S INSIDE\n",
    "\n",
    "Tweet Tweet  Share\n",
    "More On This Topic\n",
    "A Gentle Introduction to Bayes Theorem for Machine Learning\n",
    "A Gentle Introduction to Bayes Theorem for Machine Learning\n",
    "Naive Bayes Classifier From Scratch in Python\n",
    "Naive Bayes Classifier From Scratch in Python\n",
    "What Is the Naive Classifier for Each Imbalanced Classification Metric?\n",
    "What Is the Naive Classifier for Each Imbalanced…\n",
    "A Gentle Introduction to the Bayes Optimal Classifier\n",
    "A Gentle Introduction to the Bayes Optimal Classifier\n",
    "Naive Bayes for Machine Learning\n",
    "Naive Bayes for Machine Learning\n",
    "Better Naive Bayes: 12 Tips To Get The Most From The Naive Bayes Algorithm\n",
    "Better Naive Bayes: 12 Tips To Get The Most From The…\n",
    "\n",
    "About Jason Brownlee\n",
    "Jason Brownlee, PhD is a machine learning specialist who teaches developers how to get results with modern machine learning methods via hands-on tutorials.\n",
    "View all posts by Jason Brownlee →\n",
    " A Gentle Introduction to Bayes Theorem for Machine LearningHow to Implement Bayesian Optimization from Scratch in Python \n",
    "33 Responses to How to Develop a Naive Bayes Classifier from Scratch in Python\n",
    "\n",
    "Anthony The Koala October 11, 2019 at 1:34 pm #\n",
    "Dr Dr Jason,\n",
    "We’ve all seen the scatter plots of the “iris data” of sepal length vs sepal width, for the three different iris species, setosa, versicolor and virginica, an example is at https://bigqlabsdotcom.files.wordpress.com/2016/06/iris_data-scatter-plot-11.png?w=620 source https://bigqlabs.com/2016/06/27/training-a-naive-bayes-classifier-using-sklearn/ .\n",
    "\n",
    "We see in the plots where there is some overlap in lengths, that is some of versicolors’s lengths are the same as the virginica.\n",
    "\n",
    "Where there is overlap, is there any algorithm that can make a distinction between a virginica and versicolor?\n",
    "\n",
    "Put it another way, are there any additional variables needed to increase the accuracy of prediction between the three species?\n",
    "\n",
    "This is because so far relying on sepal length and sepal width is not enough.\n",
    "\n",
    "Thank you,\n",
    "Anthony of Sydney\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee October 11, 2019 at 1:53 pm #\n",
    "Perhaps you can use the available data to engineer new input features?\n",
    "\n",
    "REPLY\n",
    "\n",
    "Zahid Hasan October 11, 2019 at 2:43 pm #\n",
    "Dear Sir\n",
    "\n",
    "i am very thankful of your valued information which you send me. These are very useful for my education and I am waiting for you more coperation like this\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee October 12, 2019 at 6:45 am #\n",
    "Thanks, I’m happy the tutorials are helpful!\n",
    "\n",
    "REPLY\n",
    "\n",
    "Fidel November 2, 2019 at 12:27 am #\n",
    "Good work sir. Pls, how do I apply naive Bayes rule in predicting road traffic congestion. Eg, if I am monitoring traffic in about 3 different routes say, a, b and c. Reply me pls.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee November 2, 2019 at 6:44 am #\n",
    "I recommend following this process:\n",
    "https://machinelearningmastery.com/start-here/#process\n",
    "\n",
    "REPLY\n",
    "\n",
    "Adrian November 21, 2019 at 7:50 am #\n",
    "What is the base value of the logarithm of probabilities?\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee November 21, 2019 at 1:25 pm #\n",
    "We use add log probs to avoid multiplying many small numbers which can result in an underflow.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Queeniee January 7, 2020 at 4:05 pm #\n",
    "easier naive bayes implementation is below:\n",
    "https://github.com/Bhavya112298/Machine-Learning-algorithms-scratch-implementation\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee January 8, 2020 at 8:20 am #\n",
    "Thanks for sharing.\n",
    "\n",
    "REPLY\n",
    "\n",
    "pampam February 6, 2020 at 5:40 pm #\n",
    "hey i have question what mean fit_prior and class_prior i dont understand, i have read the documentation and i dont understand. thx before\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee February 7, 2020 at 8:09 am #\n",
    "hat don’t you understand exactly?\n",
    "\n",
    "REPLY\n",
    "\n",
    "pampam February 7, 2020 at 10:21 am #\n",
    "yes i dont understand what is mean can u make it simple or example what fit_prior and class_prior used to? sorry for bad english\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee February 7, 2020 at 1:48 pm #\n",
    "A class prior is the probability of any observation belonging to that class, give no information.\n",
    "\n",
    "No idea what you mean by the fit prior, I’ve not heard of that before.\n",
    "\n",
    "REPLY\n",
    "\n",
    "pampam February 8, 2020 at 12:09 am #\n",
    "thx for answer sir!\n",
    "\n",
    "\n",
    "Naren March 12, 2020 at 3:02 am #\n",
    "A very well written article. Well I am working on a PGM query model and was hunting around for ideas on how best to represent a CPD. Should it be a dict of dicts or dataframe etc or a combination of both where the indexes are the enumeration of the “inedges” with the possible node values being the columns. Will probably look around or if not build a custom object. And if I’m really upset I’ll do it in Java 😅\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee March 12, 2020 at 8:53 am #\n",
    "Perhaps prototype a few approaches?\n",
    "\n",
    "REPLY\n",
    "\n",
    "ustengg April 3, 2020 at 10:27 pm #\n",
    "Thank you so much for this, Sir!\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee April 4, 2020 at 6:18 am #\n",
    "You’re welcome!\n",
    "\n",
    "REPLY\n",
    "\n",
    "Catherine April 9, 2020 at 9:54 pm #\n",
    "Thank you for the tutorial. I appreciate the naive Bayes concept, but still have issues while trying to classify dataset from user ratings of products into two labels [similar ratings; dissimilar rating] using the Naive Bayes classifier.\n",
    "\n",
    "Note, am using ‘AppleStore.csv’ dataset.\n",
    "\n",
    "kindly, help, am very new in this territory.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee April 10, 2020 at 8:29 am #\n",
    "Perhaps one of the methods here will help:\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "REPLY\n",
    "\n",
    "Fawwaz May 20, 2020 at 8:35 am #\n",
    "Can it be used for 4 classes case?\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee May 20, 2020 at 1:33 pm #\n",
    "Sure.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jyothi May 30, 2020 at 10:20 pm #\n",
    "Hi Jason, Can we apply conditional probability and Bayes classifier in case of non-Gaussian distributions for class distributions? Do we have any cost functions that measure the classification model uncertainty with respect to non-Gaussian densities of the class distributions?\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee May 31, 2020 at 6:27 am #\n",
    "You can use another distribution for a feature, even a kernel or empirical distribution.\n",
    "\n",
    "Not sure about using a conditional distribution, sorry.\n",
    "\n",
    "Think of each feature/distribution as a model and naive bayes as an ensemble of these smaller models.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Vishal July 16, 2020 at 2:02 am #\n",
    "Hi Jason,\n",
    "\n",
    "I have a data set in which I have both categorical variables and numerical variables. I decided to use binomial distribution for the categorical variables(A,B,C) and gaussian distribution for the numerical variables(D,E,F).\n",
    "I don’t want to construct the naive bayes model from scratch. I there any way in sklearn or any other library in python to tell the naive bayes model that it has to treat variable A,B,C as binomial distribution and variable D,E,F as gaussian distribution?\n",
    "\n",
    "Thanks for reply\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee July 16, 2020 at 6:44 am #\n",
    "Great question.\n",
    "\n",
    "Off the cuff, I don’t think the sklearn implementations support mixed types. You could have one model for each variable type and ensemble their predictions together.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Vishal July 16, 2020 at 9:27 pm #\n",
    "Hi Jason,\n",
    "Following you I read an article on ensemble learning.\n",
    "Below is the link:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/ensemble-learning-python\n",
    "\n",
    "In this article it is written, “Also, if you need to work in a probabilistic setting, ensemble methods may not work either. ”\n",
    "\n",
    "You can check this by yourself. So ensemble method is not going to work for naive bayes. Is there any alternative to mixed types in python ?\n",
    "\n",
    "Thanks for reply.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Jason Brownlee July 17, 2020 at 6:16 am #\n",
    "I disagree with the statement and I have not read the post. Probabilistic methods are used in ensembles all the time and are effective.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Dr. Pol August 13, 2021 at 5:40 pm #\n",
    "Hi,\n",
    "Nice article\n",
    "But I think I see a problem:\n",
    "When you use the norm.pdf function, you get the distribution value y at the supplied x value – the likelihood. The height of the distribution is not normalized to 1 (the area under it, is) and this means you can get all kind of numbers that have no true relation to probability between the distributions, as one pdf value can be lower than another even though it is more likely to be a sample of the first.\n",
    "\n",
    "This will not be a hugh problem when two distribution of the same feature (for two classes) have a similar standard deviation. But if, for example, one distribution has a SD that is 10 times bigger than the 2nd one, it will return a pdf value that is 10 time lower for the a sample with the same z score.\n",
    "\n",
    "Would be happy to hear your thoughts.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Adrian Tam August 14, 2021 at 2:53 am #\n",
    "That is the nature of pdf. It is a density function (which can be any value), and shouldn’t be interpreted as probability (which is between 0 and 1). If you want a probability-like value, you may consider norm.cdf (cumulative distribution function) but you must understand what you’re doing. It is not always a direct substitution.\n",
    "\n",
    "REPLY\n",
    "\n",
    "Ernst williams February 8, 2022 at 3:57 am #\n",
    "Hi Jason, very thankful for the valuable information you have shared in the article. The concepts proved very helpful still waiting for more content like this. The tutorials are helpful. I appreciate the naive Bayes concept. I have also gone through https://www.edvanza.com/ perhaps one of the methods here will help.\n",
    "\n",
    "REPLY\n",
    "\n",
    "James Carmichael February 8, 2022 at 12:33 pm #\n",
    "Thank you for the feedback and suggestion Ernst!\n",
    "\n",
    "REPLY\n",
    "Leave a Reply\n",
    "\n",
    "Name (required)\n",
    "\n",
    "Email (will not be published) (required)\n",
    "\n",
    "\n",
    "Welcome!\n",
    "I'm Jason Brownlee PhD\n",
    "and I help developers get results with machine learning.\n",
    "Read more\n",
    "\n",
    "Never miss a tutorial:\n",
    "\n",
    "LinkedIn     Twitter     Facebook     Email Newsletter     RSS Feed\n",
    "Picked for you:\n",
    "\n",
    "ROC Curve Plot for a No Skill Classifier and a Logistic Regression Model\n",
    "How to Use ROC Curves and Precision-Recall Curves for Classification in Python\n",
    "Calibrated and Uncalibrated SVM Reliability Diagram\n",
    "How and When to Use a Calibrated Classification Model with scikit-learn\n",
    "Plot of The Input Samples Evaluated with a Noisy (dots) and Non-Noisy (Line) Objective Function\n",
    "How to Implement Bayesian Optimization from Scratch in Python\n",
    "Histogram of Two Different Probability Distributions for the Same Random Variable\n",
    "How to Calculate the KL Divergence for Machine Learning\n",
    "Line Plot of Probability Distribution vs Cross-Entropy for a Binary Classification Task With Extreme Case Removed\n",
    "A Gentle Introduction to Cross-Entropy for Machine Learning\n",
    "Loving the Tutorials?\n",
    "The Probability for Machine Learning EBook is where you'll find the Really Good stuff.\n",
    "\n",
    ">> SEE WHAT'S INSIDE\n",
    "\n",
    "© 2022 Machine Learning Mastery. All Rights Reserved.\n",
    "LinkedIn | Twitter | Facebook | Newsletter | RSS\n",
    "\n",
    "Privacy | Disclaimer | Terms | Contact | Sitemap | Search\n",
    "\n",
    "Start Machine Learning\n",
    "×\n",
    "Start Machine Learning\n",
    "You can master applied Machine Learning \n",
    "without math or fancy degrees.\n",
    "Find out how in this free and practical course.\n",
    "Email Address\n",
    " I consent to receive information about services and special offers by email. For more information, see the Privacy Policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('prac-weak')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3239d9b19dc71823afce0461a5421d11a9d24253957a55b30a6bf76803c2fa56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
