{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python"
  },
  "interpreter": {
   "hash": "c28c176a6561e0f599baf12f61c08a38880cae9f7d0fc7d3751de036b3a31dd1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Snorkel-labeled Dataset for Text Classification\n",
    "\n",
    "In the previous section, we showed how you can use _ktrain_ (a  wrapper for TensorFlow and Hugging Face Transformers) to perform text classification.\n",
    "_ktrain_ helps anyone new to text classification jumpstart the use of transformer-based models, and enables people to get started quickly.\n",
    "\n",
    "As you get familiar with using transformer-based models, you might want to leverage the full capabilities of the Hugging Face Python library directly.  \n",
    "In this notebook, we show you how you can use Hugging Face and one of the state-of-art transformers in Hugging Face called _RoBERTa_.\n",
    "\n",
    "_RoBERTa_ uses a similar architecture to _BERT_, and uses a byte-level BPE as a tokenizer.  _RoBERTa_ made several other optimizations to improve the _BERT_ architecture. These include bigger batch size, longer training time, and using more diversified training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "source": [
    "## Load relevant Python packages\n",
    "Let us get started by loading the relevant _Hugging Face Transformer libraries_, and _sklearn_. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers as tfs\n"
   ]
  },
  {
   "source": [
    "### Checking availability of GPUs and type of GPU\n",
    "Determine the number of available GPUs that can be used for training. In addition, we also print out the type of GPU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU(s) available: 1 \nDevice: NVIDIA Tesla K80\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'GPU(s) available: {torch.cuda.device_count()} ')\n",
    "    print('Device:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPUs available. Default to use CPU')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id                                          statement  snorkel_labels\n",
       "0   1248  During the Reagan era, while productivity incr...               1\n",
       "1   4518  \"It costs more money to put a person on death ...               1\n",
       "2  15972  Says that Minnesota Democratic congressional c...               0\n",
       "3  11165  \"This is the most generous country in the worl...               1\n",
       "4  14618  \"Loopholes in current law prevent ‘Unaccompani...               0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>statement</th>\n      <th>snorkel_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1248</td>\n      <td>During the Reagan era, while productivity incr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4518</td>\n      <td>\"It costs more money to put a person on death ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15972</td>\n      <td>Says that Minnesota Democratic congressional c...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11165</td>\n      <td>\"This is the most generous country in the worl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14618</td>\n      <td>\"Loopholes in current law prevent ‘Unaccompani...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Read the Fake News Dataset and show the first few rows\n",
    "fakenews_df = pd.read_csv('../data/fakenews_snorkel_labels.csv')\n",
    "fakenews_df[['id', 'statement','snorkel_labels']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fakenews_df['spk_statement'] = fakenews_df['speaker'] + \" \" + fakenews_df['statement'] \n",
    "fakenews_df['spk_statement'] = fakenews_df['statement'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    During the Reagan era, while productivity incr...\n",
       "1    \"It costs more money to put a person on death ...\n",
       "2    Says that Minnesota Democratic congressional c...\n",
       "3    \"This is the most generous country in the worl...\n",
       "4    \"Loopholes in current law prevent ‘Unaccompani...\n",
       "Name: spk_statement, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "fakenews_df['spk_statement'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fakenews_df.loc[:,['spk_statement', 'snorkel_labels']]\n",
    "labels = X.snorkel_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1,  0, -1])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Get the unique labels \n",
    "categories = labels.unique()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X[X['snorkel_labels'] >= 0] \n",
    "X = X[X['snorkel_labels'] >= 0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    6287\n",
       "0    5859\n",
       "Name: snorkel_labels, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# count each of the labels\n",
    "X['snorkel_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['spk_statement'], X['snorkel_labels'], test_size=0.20, random_state=122520)\n",
    "\n",
    "# withold test cases for testing\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.30, random_state=122420)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test set: 1701\nVal set: 729\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set: %d\" % len(X_test))\n",
    "print(\"Val set: %d\" % len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11974    Says North Carolina Rep. Renee Ellmers was the...\n",
       "13124             Says coronavirus is just “the damn flu.”\n",
       "9872     \"54 senators said, let's do background checks,...\n",
       "1398     Says U.S. Rep. Justin Amash \"votes more with N...\n",
       "6068     \"We are accepting more legal immigrants than w...\n",
       "Name: spk_statement, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    5028\n",
       "0    4688\n",
       "Name: snorkel_labels, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    374\n",
       "0    355\n",
       "Name: snorkel_labels, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    885\n",
       "0    816\n",
       "Name: snorkel_labels, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rows in X_train 9716 : \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    5028\n",
       "0    4688\n",
       "Name: snorkel_labels, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Count of label 0 and 1 in the training data set\n",
    "print(\"Rows in X_train %d : \" % len(X_train))\n",
    "y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\"Unemployment has fallen from 10 percent during the president’s first year in office to 8.5 percent today.\"'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "X_train[5568]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rows in X_train 729 : \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    374\n",
       "0    355\n",
       "Name: snorkel_labels, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "print(\"Rows in X_train %d : \" % len(X_val))\n",
    "y_val.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset - Tokenization\n",
    "Perform tokenization using the RoBERTa tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Load the pre-trained roberta-base model using _RobertaForSequenceClassification.from_pretrained(...)_.\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"Unemployment has fallen from 10 percent during th\ninputs_ids\n[[0, 113, 2], [0, 791, 2], [0, 282, 2], [0, 242, 2], [0, 119, 2], [0, 642, 2], [0, 462, 2], [0, 139, 2], [0, 219, 2], [0, 119, 2], [0, 242, 2], [0, 282, 2], [0, 90, 2], [0, 2, 1], [0, 298, 2], [0, 102, 2], [0, 29, 2], [0, 2, 1], [0, 506, 2], [0, 102, 2], [0, 462, 2], [0, 462, 2], [0, 242, 2], [0, 282, 2], [0, 2, 1], [0, 506, 2], [0, 338, 2], [0, 139, 2], [0, 119, 2], [0, 2, 1], [0, 134, 2], [0, 288, 2], [0, 2, 1], [0, 642, 2], [0, 242, 2], [0, 338, 2], [0, 438, 2], [0, 242, 2], [0, 282, 2], [0, 90, 2], [0, 2, 1], [0, 417, 2], [0, 257, 2], [0, 338, 2], [0, 118, 2], [0, 282, 2], [0, 571, 2], [0, 2, 1], [0, 90, 2], [0, 298, 2]]\n\nAttention mask\n[[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Test code for Tokenizer\n",
    "text = X_train[5568][:50]\n",
    "print(text)\n",
    "\n",
    "# encode text\n",
    "encoded_text = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
    "print(\"inputs_ids\")\n",
    "print(encoded_text[\"input_ids\"])\n",
    "\n",
    "print(\"\\nAttention mask\")\n",
    "print(encoded_text[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 31
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 381.65 248.518125\" width=\"381.65pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-06-16T01:03:24.504353</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 248.518125 \nL 381.65 248.518125 \nL 381.65 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \nL 374.45 7.2 \nL 39.65 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 54.868182 224.64 \nL 67.042727 224.64 \nL 67.042727 216.53161 \nL 54.868182 216.53161 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 67.042727 224.64 \nL 79.217273 224.64 \nL 79.217273 155.911739 \nL 67.042727 155.911739 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 79.217273 224.64 \nL 91.391818 224.64 \nL 91.391818 54.363804 \nL 79.217273 54.363804 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 91.391818 224.64 \nL 103.566364 224.64 \nL 103.566364 75.600064 \nL 91.391818 75.600064 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 103.566364 224.64 \nL 115.740909 224.64 \nL 115.740909 17.554286 \nL 103.566364 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 115.740909 224.64 \nL 127.915455 224.64 \nL 127.915455 44.19614 \nL 115.740909 44.19614 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 127.915455 224.64 \nL 140.09 224.64 \nL 140.09 78.688975 \nL 127.915455 78.688975 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 140.09 224.64 \nL 152.264545 224.64 \nL 152.264545 150.377441 \nL 140.09 150.377441 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 152.264545 224.64 \nL 164.439091 224.64 \nL 164.439091 142.011642 \nL 152.264545 142.011642 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 164.439091 224.64 \nL 176.613636 224.64 \nL 176.613636 167.10904 \nL 164.439091 167.10904 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 176.613636 224.64 \nL 188.788182 224.64 \nL 188.788182 195.938872 \nL 176.613636 195.938872 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 188.788182 224.64 \nL 200.962727 224.64 \nL 200.962727 195.424054 \nL 188.788182 195.424054 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 200.962727 224.64 \nL 213.137273 224.64 \nL 213.137273 204.690786 \nL 200.962727 204.690786 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 213.137273 224.64 \nL 225.311818 224.64 \nL 225.311818 213.056585 \nL 213.137273 213.056585 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 225.311818 224.64 \nL 237.486364 224.64 \nL 237.486364 220.264043 \nL 225.311818 220.264043 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 237.486364 224.64 \nL 249.660909 224.64 \nL 249.660909 219.62052 \nL 237.486364 219.62052 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 249.660909 224.64 \nL 261.835455 224.64 \nL 261.835455 222.194612 \nL 249.660909 222.194612 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 261.835455 224.64 \nL 274.01 224.64 \nL 274.01 223.224249 \nL 261.835455 223.224249 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 274.01 224.64 \nL 286.184545 224.64 \nL 286.184545 223.224249 \nL 274.01 223.224249 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 286.184545 224.64 \nL 298.359091 224.64 \nL 298.359091 223.739068 \nL 286.184545 223.739068 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 298.359091 224.64 \nL 310.533636 224.64 \nL 310.533636 224.253886 \nL 298.359091 224.253886 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 310.533636 224.64 \nL 322.708182 224.64 \nL 322.708182 223.996477 \nL 310.533636 223.996477 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 322.708182 224.64 \nL 334.882727 224.64 \nL 334.882727 224.511295 \nL 322.708182 224.511295 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 334.882727 224.64 \nL 347.057273 224.64 \nL 347.057273 224.511295 \nL 334.882727 224.511295 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p7e3891487d)\" d=\"M 347.057273 224.64 \nL 359.231818 224.64 \nL 359.231818 224.511295 \nL 347.057273 224.511295 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 41.440374 224.64 \nL 41.440374 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m233035bc92\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"41.440374\" xlink:href=\"#m233035bc92\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(38.259124 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 86.199733 224.64 \nL 86.199733 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.199733\" xlink:href=\"#m233035bc92\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(79.837233 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 130.959091 224.64 \nL 130.959091 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.959091\" xlink:href=\"#m233035bc92\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(124.596591 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 175.718449 224.64 \nL 175.718449 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"175.718449\" xlink:href=\"#m233035bc92\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g transform=\"translate(169.355949 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 220.477807 224.64 \nL 220.477807 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"220.477807\" xlink:href=\"#m233035bc92\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g transform=\"translate(214.115307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 265.237166 224.64 \nL 265.237166 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"265.237166\" xlink:href=\"#m233035bc92\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g transform=\"translate(258.874666 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 309.996524 224.64 \nL 309.996524 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.996524\" xlink:href=\"#m233035bc92\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <g transform=\"translate(303.634024 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 354.755882 224.64 \nL 354.755882 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"354.755882\" xlink:href=\"#m233035bc92\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 70 -->\n      <g transform=\"translate(348.393382 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me7dcde00a9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(26.2875 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 198.899078 \nL 374.45 198.899078 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"198.899078\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 200 -->\n      <g transform=\"translate(13.5625 202.698297)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 173.158157 \nL 374.45 173.158157 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"173.158157\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 400 -->\n      <g transform=\"translate(13.5625 176.957376)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 147.417235 \nL 374.45 147.417235 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"147.417235\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 600 -->\n      <g transform=\"translate(13.5625 151.216454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 121.676314 \nL 374.45 121.676314 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"121.676314\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 800 -->\n      <g transform=\"translate(13.5625 125.475532)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 95.935392 \nL 374.45 95.935392 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"95.935392\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1000 -->\n      <g transform=\"translate(7.2 99.734611)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 70.19447 \nL 374.45 70.19447 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"70.19447\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1200 -->\n      <g transform=\"translate(7.2 73.993689)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 44.453549 \nL 374.45 44.453549 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"44.453549\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 1400 -->\n      <g transform=\"translate(7.2 48.252768)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p7e3891487d)\" d=\"M 39.65 18.712627 \nL 374.45 18.712627 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#me7dcde00a9\" y=\"18.712627\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 1600 -->\n      <g transform=\"translate(7.2 22.511846)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 39.65 224.64 \nL 39.65 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 374.45 224.64 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 39.65 7.2 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7e3891487d\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXb0lEQVR4nO3db4xc9Xn28e9V0ziGKWBq2Fpeq3Yql9ZmkzTeuk5pq9mQFjcgzIuHahFpTEW1KnLTpHLU2o1U1BdWrba0JUpBWq0pRkS4rpPWbvKQBLkZoecRjoOBdLGJi1uvyGLHThogbEpd1rn7Yn5uhmV2dufvzvC7PtJqz9zn3+VhuOfsb86co4jAzMzy8CMLHcDMzDrHTd/MLCNu+mZmGXHTNzPLiJu+mVlGLlnoAHNZtmxZXH311Vx22WULHaUu3//+9525A5y5/XotLzgzwNGjR78TEVe/ZUZEdPXP+vXr4ytf+Ur0GmfuDGduv17LG+HMERHAU1Glp3p4x8wsI276ZmYZcdM3M8uIm76ZWUbc9M3MMuKmb2aWETd9M7OMzNn0JT0o6Zyk52bUPyrphKRjkv6sor5D0sk078aK+npJ42nepySptf8UMzOby3yO9B8CNlUWJA0Bm4F3R8Q64C9SfS0wDKxL69wvaVFa7QFgBFiTft60TTMza785L8MQEU9IWjWjfDewKyLOp2XOpfpmYG+qn5J0EtggaQK4PCKeBJD0MHAr8Fgr/hFvZ6u2f6Gu5Sd23dSmJGb2dtDotXd+GvhlSTuB/wI+ERFfA1YAhyuWm0y1N9L0zHpVkkYo/1VAX18fU1NTlEqlBqMujFZl3jYwXdfyzewz5+e5k3otc6/lBWeupdGmfwmwFNgI/DywT9K7gGrj9FGjXlVEjAKjAIODg1EoFCgWiw1GXRilUqklme+s90j/jsb32arMneTM7ddrecGZa2n07J1J4HPpuj5HgB8Ay1J9ZcVy/cDpVO+vUjczsw5qtOn/I/ABAEk/DbwD+A5wEBiWtFjSasof2B6JiDPAa5I2prN2PgIcaDa8mZnVZ87hHUmPAkVgmaRJ4B7gQeDBdBrnfwNb0qU8j0naBxwHpoGtEXEhbepuymcCLaH8Aa4/xDUz67D5nL1z+yyzPjzL8juBnVXqTwHX1ZXOzMxayt/INTPLiJu+mVlG3PTNzDLipm9mlhE3fTOzjLjpm5llxE3fzCwjbvpmZhlx0zczy4ibvplZRtz0zcwy4qZvZpYRN30zs4y46ZuZZcRN38wsI276ZmYZmbPpS3pQ0rl0l6yZ8z4hKSQtq6jtkHRS0glJN1bU10saT/M+lW6baGZmHTSfI/2HgE0zi5JWAr8KvFhRWwsMA+vSOvdLWpRmPwCMUL5v7ppq2zQzs/aas+lHxBPAd6vM+ivgD4CoqG0G9kbE+Yg4BZwENkhaDlweEU+me+k+DNzabHgzM6vPnPfIrUbSLcBLEfH1GaM0K4DDFY8nU+2NND2zbi22avsX6lp+YtdNbUpiZt2o7qYv6VLgk8CvVZtdpRY16rPtY4TyUBB9fX1MTU1RKpXqjbqgWpV528B082FqqMyY8/PcSb2WudfygjPX0siR/k8Bq4GLR/n9wNOSNlA+gl9ZsWw/cDrV+6vUq4qIUWAUYHBwMAqFAsVisYGoC6dUKrUk8511HrnXa+KO4v9OtypzJzlz+/VaXnDmWuo+ZTMixiPimohYFRGrKDf090XEt4CDwLCkxZJWU/7A9khEnAFek7QxnbXzEeBA6/4ZZmY2H/M5ZfNR4EngWkmTku6abdmIOAbsA44DXwS2RsSFNPtuYIzyh7v/BjzWZHYzM6vTnMM7EXH7HPNXzXi8E9hZZbmngOvqzGdmZi3kb+SamWXETd/MLCNu+mZmGXHTNzPLiJu+mVlGGroMg5XVuuTBtoHpt3yxypc8MLOF5iN9M7OMuOmbmWXETd/MLCNu+mZmGXHTNzPLiJu+mVlG3PTNzDLipm9mlhE3fTOzjLjpm5llxE3fzCwj87ld4oOSzkl6rqL255K+IelfJP2DpCsr5u2QdFLSCUk3VtTXSxpP8z6V7pVrZmYdNJ8j/YeATTNqjwPXRcS7gX8FdgBIWgsMA+vSOvdLWpTWeQAYoXyz9DVVtmlmZm02Z9OPiCeA786ofTkiptPDw0B/mt4M7I2I8xFxivJN0DdIWg5cHhFPRkQADwO3tujfYGZm86RyD55jIWkV8PmIeMuNzSX9E/B3EfGIpE8DhyPikTRvN/AYMAHsiogPpvovA38YETfPsr8Ryn8V0NfXt35sbIxCodDAP6+9xl96ddZ5fUvg7Otvrg2suKKl+2iFykxTU1Nd+TzX4szt12t5wZkBhoaGjkbE4Mx6U9fTl/RJYBr4zMVSlcWiRr2qiBgFRgEGBwejUChQLBabidoWM6+XX2nbwDT3jr/56Z24o9jSfbRCZaZSqdSVz3Mtztx+vZYXnLmWhpu+pC3AzcAN8cM/FyaBlRWL9QOnU72/St3MzDqooVM2JW0C/hC4JSL+s2LWQWBY0mJJqyl/YHskIs4Ar0namM7a+QhwoMnsZmZWpzmP9CU9ChSBZZImgXson62zGHg8nXl5OCJ+JyKOSdoHHKc87LM1Ii6kTd1N+UygJZTH+R9r7T/FzMzmMmfTj4jbq5R311h+J7CzSv0p4C0fBJuZWef4xugdVOtG6mZmneDLMJiZZcRN38wsIx7eyVzlkNO2gek5vxcwseumdkcyszbykb6ZWUbc9M3MMuKmb2aWETd9M7OMuOmbmWXETd/MLCNu+mZmGXHTNzPLiJu+mVlG3PTNzDLipm9mlhE3fTOzjLjpm5llZM6mL+lBSeckPVdRu0rS45JeSL+XVszbIemkpBOSbqyor5c0nuZ9Kt0r18zMOmg+R/oPAZtm1LYDhyJiDXAoPUbSWmAYWJfWuV/SorTOA8AI5Zulr6myTTMza7M5m35EPAF8d0Z5M7AnTe8Bbq2o742I8xFxCjgJbJC0HLg8Ip6MiAAerljHzMw6pNGbqPRFxBmAiDgj6ZpUXwEcrlhuMtXeSNMz61VJGqH8VwF9fX1MTU1RKpUajNo+2wamZ53Xt6T2/G40n8zd9t+hW18btfRa5l7LC85cS6vvnFVtnD5q1KuKiFFgFGBwcDAKhQLFYrElAVup1l2mtg1Mc+94b92YbD6ZJ+4odibMPJVKpa58bdTSa5l7LS84cy2Nnr1zNg3ZkH6fS/VJYGXFcv3A6VTvr1I3M7MOarTpHwS2pOktwIGK+rCkxZJWU/7A9kgaCnpN0sZ01s5HKtYxM7MOmXP8QdKjQBFYJmkSuAfYBeyTdBfwInAbQEQck7QPOA5MA1sj4kLa1N2UzwRaAjyWfszMrIPmbPoRcfsss26YZfmdwM4q9aeA6+pKZ2ZmLeVv5JqZZcRN38wsI276ZmYZcdM3M8uIm76ZWUbc9M3MMuKmb2aWETd9M7OMuOmbmWXETd/MLCNu+mZmGXHTNzPLiJu+mVlG3PTNzDLipm9mlhE3fTOzjLjpm5llpKmmL+n3JR2T9JykRyW9U9JVkh6X9EL6vbRi+R2STko6IenG5uObmVk9Gm76klYAvwcMRsR1wCJgGNgOHIqINcCh9BhJa9P8dcAm4H5Ji5qLb2Zm9Wh2eOcSYImkS4BLgdPAZmBPmr8HuDVNbwb2RsT5iDgFnAQ2NLl/MzOrgyKi8ZWlj1G+CfrrwJcj4g5Jr0TElRXLvBwRSyV9GjgcEY+k+m7gsYjYX2W7I8AIQF9f3/qxsTEKhULDOdtl/KVXZ53XtwTOvt7BMC0wn8wDK67oTJh5mpqa6srXRi29lrnX8oIzAwwNDR2NiMGZ9Usa3WAaq98MrAZeAf5e0odrrVKlVvUdJyJGgVGAwcHBKBQKFIvFRqO2zZ3bvzDrvG0D09w73vDTuyDmk3nijmJnwsxTqVTqytdGLb2WudfygjPX0szwzgeBUxHx7Yh4A/gc8IvAWUnLAdLvc2n5SWBlxfr9lIeDzMysQ5pp+i8CGyVdKknADcDzwEFgS1pmC3AgTR8EhiUtlrQaWAMcaWL/ZmZWp4bHHyLiq5L2A08D08AzlIdkCsA+SXdRfmO4LS1/TNI+4HhafmtEXGgyv5mZ1aGpQeeIuAe4Z0b5POWj/mrL76T8wa+ZmS0AfyPXzCwjbvpmZhlx0zczy4ibvplZRtz0zcwy4qZvZpYRN30zs4y46ZuZZcRN38wsI276ZmYZcdM3M8uIm76ZWUbc9M3MMuKmb2aWkd66n5/1nFU1bilZzcSum9qUxMzAR/pmZllpqulLulLSfknfkPS8pPdLukrS45JeSL+XViy/Q9JJSSck3dh8fDMzq0ezR/r3AV+MiJ8B3kP5HrnbgUMRsQY4lB4jaS0wDKwDNgH3S1rU5P7NzKwODTd9SZcDvwLsBoiI/46IV4DNwJ602B7g1jS9GdgbEecj4hRwEtjQ6P7NzKx+zRzpvwv4NvC3kp6RNCbpMqAvIs4ApN/XpOVXAN+sWH8y1czMrEMUEY2tKA0Ch4HrI+Krku4Dvgd8NCKurFju5YhYKulvgCcj4pFU3w3834j4bJVtjwAjAH19fevHxsYoFAoN5Wyn8ZdenXVe3xI4+3oHw7TAfDIPrLiirm3Weo5asf2pqamufG3U0muZey0vODPA0NDQ0YgYnFlv5pTNSWAyIr6aHu+nPH5/VtLyiDgjaTlwrmL5lRXr9wOnq204IkaBUYDBwcEoFAoUi8UmorbHnTVOR9w2MM294711Rux8Mk/cUaxrm7Weo1Zsv1QqdeVro5Zey9xrecGZa2l4eCcivgV8U9K1qXQDcBw4CGxJtS3AgTR9EBiWtFjSamANcKTR/ZuZWf2aPRT9KPAZSe8A/h34LcpvJPsk3QW8CNwGEBHHJO2j/MYwDWyNiAtN7t86rN4vW5lZd2mq6UfEs8BbxowoH/VXW34nsLOZfZqZWeP8jVwzs4y46ZuZZcRN38wsI711TmGb+UNKM3u785G+mVlG3PTNzDLipm9mlhE3fTOzjLjpm5llxE3fzCwjbvpmZhlx0zczy4ibvplZRtz0zcwy4sswWFep91IY2wamKbYnitnbko/0zcwy4qZvZpaRppu+pEWSnpH0+fT4KkmPS3oh/V5asewOSSclnZB0Y7P7NjOz+rTiSP9jwPMVj7cDhyJiDXAoPUbSWmAYWAdsAu6XtKgF+zczs3lqqulL6gduAsYqypuBPWl6D3BrRX1vRJyPiFPASWBDM/s3M7P6KCIaX1naD/wp8GPAJyLiZkmvRMSVFcu8HBFLJX0aOBwRj6T6buCxiNhfZbsjwAhAX1/f+rGxMQqFQsM552v8pVdbtq2+JXD29ZZtriN6NfM1V12x0DHqMjU11ZHXc6v0Wl5wZoChoaGjETE4s97wKZuSbgbORcRRScX5rFKlVvUdJyJGgVGAwcHBKBQKFIvz2UVz7mzhnbO2DUxz73hvnRHbq5l/owOvjVYqlUodeT23Sq/lBWeupZn/w68HbpH0IeCdwOWSHgHOSloeEWckLQfOpeUngZUV6/cDp5vYv5mZ1anhMf2I2BER/RGxivIHtP8cER8GDgJb0mJbgANp+iAwLGmxpNXAGuBIw8nNzKxu7fhbfhewT9JdwIvAbQARcUzSPuA4MA1sjYgLbdi/mZnNoiVNPyJKQClN/wdwwyzL7QR2tmKfZmZWP38j18wsI276ZmYZcdM3M8uIm76ZWUbc9M3MMuKmb2aWETd9M7OMuOmbmWXETd/MLCNu+mZmGXHTNzPLiJu+mVlG3PTNzDLipm9mlhE3fTOzjLjpm5llxE3fzCwjDd85S9JK4GHgJ4AfAKMRcZ+kq4C/A1YBE8BvRMTLaZ0dwF3ABeD3IuJLTaU3A1Zt/0Jbtz+x66a2bt+sk5o50p8GtkXEzwIbga2S1gLbgUMRsQY4lB6T5g0D64BNwP2SFjUT3szM6tNw04+IMxHxdJp+DXgeWAFsBvakxfYAt6bpzcDeiDgfEaeAk8CGRvdvZmb1U0Q0vxFpFfAEcB3wYkRcWTHv5YhYKunTwOGIeCTVdwOPRcT+KtsbAUYA+vr61o+NjVEoFJrOOZfxl15t2bb6lsDZ11u2uY5w5uoGVlzR0u1NTU115PXcKr2WF5wZYGho6GhEDM6sNzymf5GkAvBZ4OMR8T1Jsy5apVb1HSciRoFRgMHBwSgUChSLxWajzunOFo4NbxuY5t7xpp/ejnLm6ibuKLZ0e6VSqSOv51bptbzgzLU0dfaOpB+l3PA/ExGfS+Wzkpan+cuBc6k+CaysWL0fON3M/s3MrD4NN32VD+l3A89HxF9WzDoIbEnTW4ADFfVhSYslrQbWAEca3b+ZmdWvmb+Lrwd+ExiX9Gyq/RGwC9gn6S7gReA2gIg4JmkfcJzymT9bI+JCE/s3M7M6Ndz0I+L/UX2cHuCGWdbZCexsdJ9mZtac3vrUzmwB1PvlL3+Zy7qZL8NgZpYRN30zs4y46ZuZZcRN38wsI276ZmYZcdM3M8uIm76ZWUZ8nr5Zi811Xv+2gek3XdzP5/VbJ/lI38wsI276ZmYZcdM3M8vI23pMv903zDYz6zU+0jczy4ibvplZRt7WwztmvaATw5A+LdQu8pG+mVlGOn6kL2kTcB+wCBiLiF2dzmCWG98Ixi7qaNOXtAj4G+BXgUnga5IORsTxTuYws9oq3yRmfoO4FfymsnA6faS/ATgZEf8OIGkvsJnyzdLNLBPt/hyjkTeqXN6IFBGd25n0f4BNEfHb6fFvAr8QEb87Y7kRYCQ9vBb4D+A7HQvaGstw5k5w5vbrtbzgzAA/GRFXzyx2+khfVWpvedeJiFFg9H9Xkp6KiMF2Bms1Z+4MZ26/XssLzlxLp8/emQRWVjzuB053OIOZWbY63fS/BqyRtFrSO4Bh4GCHM5iZZaujwzsRMS3pd4EvUT5l88GIODaPVUfnXqTrOHNnOHP79VpecOZZdfSDXDMzW1j+Rq6ZWUbc9M3MMtL1TV/SJkknJJ2UtH2h81Qj6UFJ5yQ9V1G7StLjkl5Iv5cuZMZKklZK+oqk5yUdk/SxVO/mzO+UdETS11PmP0n1rs18kaRFkp6R9Pn0uKszS5qQNC7pWUlPpVrXZpZ0paT9kr6RXtPv7/K816bn9uLP9yR9vFOZu7rpV1y24deBtcDtktYubKqqHgI2zahtBw5FxBrgUHrcLaaBbRHxs8BGYGt6Xrs583ngAxHxHuC9wCZJG+nuzBd9DHi+4nEvZB6KiPdWnDfezZnvA74YET8DvIfyc921eSPiRHpu3wusB/4T+Ac6lTkiuvYHeD/wpYrHO4AdC51rlqyrgOcqHp8Alqfp5cCJhc5YI/sBytdD6onMwKXA08AvdHtmyt9FOQR8APh8L7w2gAlg2YxaV2YGLgdOkU5K6fa8VfL/GvD/O5m5q4/0gRXANyseT6ZaL+iLiDMA6fc1C5ynKkmrgJ8DvkqXZ07DJM8C54DHI6LrMwN/DfwB8IOKWrdnDuDLko6mS6JA92Z+F/Bt4G/TENqYpMvo3rwzDQOPpumOZO72pj+vyzZYYyQVgM8CH4+I7y10nrlExIUo/0ncD2yQdN0CR6pJ0s3AuYg4utBZ6nR9RLyP8rDqVkm/stCBargEeB/wQET8HPB9umgop5b0BdVbgL/v5H67ven38mUbzkpaDpB+n1vgPG8i6UcpN/zPRMTnUrmrM18UEa8AJcqfo3Rz5uuBWyRNAHuBD0h6hO7OTEScTr/PUR5r3kD3Zp4EJtNffQD7Kb8JdGveSr8OPB0RZ9PjjmTu9qbfy5dtOAhsSdNbKI+bdwVJAnYDz0fEX1bM6ubMV0u6Mk0vAT4IfIMuzhwROyKiPyJWUX7t/nNEfJguzizpMkk/dnGa8pjzc3Rp5oj4FvBNSdem0g2UL9XelXlnuJ0fDu1ApzIv9AcZ8/ig40PAvwL/BnxyofPMkvFR4AzwBuUjj7uAH6f8Ad4L6fdVC52zIu8vUR4m+xfg2fTzoS7P/G7gmZT5OeCPU71rM8/IX+SHH+R2bWbKY+RfTz/HLv4/1+WZ3ws8lV4b/wgs7ea8KfOllC8Zf0VFrSOZfRkGM7OMdPvwjpmZtZCbvplZRtz0zcwy4qZvZpYRN30zs4y46ZuZZcRN38wsI/8D82tI5pLInOYAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Distribution of length of news\n",
    "seq_len = [len(news.split()) for news in X_train]\n",
    "pd.Series(seq_len).hist(bins = 25)"
   ]
  },
  {
   "source": [
    "Note: Changes between v2.x to v3.x HuggineFace\n",
    "\n",
    "Refer to https://discuss.huggingface.co/t/migration-guide-from-v2-x-to-v3-x-for-the-tokenizer-api/55"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Tokenization of the training and test dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Distilbert Tokenizer to tokenize and encode \n",
    "\n",
    "max_length = 256\n",
    "\n",
    "tokenized_train = tokenizer(\n",
    "    X_train.to_list(),\n",
    "    padding='max_length',\n",
    "    max_length = max_length,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_validation = tokenizer(\n",
    "    X_val.to_list(),\n",
    "    padding='max_length',\n",
    "    max_length = max_length,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "source": [
    "Next, we will convert the tokenized input_ids, attention mask, and labels into Tensors that we can use as inputs to training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map integers to Tensors\n",
    "train_input_ids_tensor = torch.tensor(tokenized_train[\"input_ids\"])\n",
    "train_attention_mask_tensor = torch.tensor(tokenized_train[\"attention_mask\"])\n",
    "train_labels_tensor = torch.tensor(y_train.to_list())\n",
    "\n",
    "\n",
    "val_input_ids_tensor = torch.tensor(tokenized_validation [\"input_ids\"])\n",
    "val_attention_mask_tensor = torch.tensor(tokenized_validation [\"attention_mask\"])\n",
    "val_labels_tensor = torch.tensor(y_val.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[    0,   104,  4113,  ...,     1,     1,     1],\n        [    0,   104,  4113,  ...,     1,     1,     1],\n        [    0,   113,  4283,  ...,     1,     1,     1],\n        ...,\n        [    0,   104,  4113,  ...,     1,     1,     1],\n        [    0,   113, 14696,  ...,     1,     1,     1],\n        [    0,   104,  4113,  ...,     1,     1,     1]])\ntensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])\ntensor([0, 0, 1,  ..., 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Show the converted tensors\n",
    "print(train_input_ids_tensor)\n",
    "print(train_attention_mask_tensor)\n",
    "print(train_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9716\n9716\n9716\n"
     ]
    }
   ],
   "source": [
    "print (len(train_labels_tensor))\n",
    "print (len(train_attention_mask_tensor))\n",
    "print (len(train_input_ids_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "729\n729\n729\n"
     ]
    }
   ],
   "source": [
    "print (len(val_labels_tensor))\n",
    "print (len(val_attention_mask_tensor))\n",
    "print (len(val_input_ids_tensor))"
   ]
  },
  {
   "source": [
    "### Converting to a TensorDataset\n",
    "Before we start to fine-tune the RoBERTa model, we will create the DataLoader for both the training and validation data. The DataLoader will be used during the fine-tuning of the model. To do this, we first convert the inputs_ids, attention_mask, and labels to a _TensorDataset_. Next, we create the DataLoader using the TensorDataset as inputs and specify the batch size. We set the variable _batch_size_ to be 16."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the DataLaoders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Specify a batch size of 32\n",
    "#batch_size = 32\n",
    "batch_size = 16\n",
    "\n",
    "# 1. Create a Tensor Datset\n",
    "# 2. Define the data sampling approach\n",
    "# 3. Create the DataLoader\n",
    "train_data_tensor = TensorDataset(train_input_ids_tensor, train_attention_mask_tensor, train_labels_tensor)\n",
    "train_dataloader = DataLoader(train_data_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_data_tensor = TensorDataset(val_input_ids_tensor, val_attention_mask_tensor, val_labels_tensor)\n",
    "val_dataloader = DataLoader(val_data_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Batch    50  of    608.\n  Batch   100  of    608.\n  Batch   150  of    608.\n  Batch   200  of    608.\n  Batch   250  of    608.\n  Batch   300  of    608.\n  Batch   350  of    608.\n  Batch   400  of    608.\n  Batch   450  of    608.\n  Batch   500  of    608.\n  Batch   550  of    608.\n  Batch   600  of    608.\n"
     ]
    }
   ],
   "source": [
    " for step,batch in enumerate(train_dataloader):\n",
    "     if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))"
   ]
  },
  {
   "source": [
    "## Model Training "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_epocs = 2\n",
    "total_steps = num_epocs * len(train_dataloader)  "
   ]
  },
  {
   "source": [
    "### Using the pre-trained RobertaForSequenceClassification model\n",
    "Next, we specify the optimizer that will be used. For this exercise, we will use the _AdamW_ optimizer, which is part of the _HuggingFace_ optimization module. The _AdamW_ optimizer implements the Adam algorithm with the weight decay fix that can be used when fine-tuning models.\n",
    "\n",
    "In addition, you will notice that we specified a scheduler, using  _get_linear_schedule_with_warmup()_. This creates a schedule with a learning rate that decreases linearly, using the initial learning rate that was set in the optimizer as the reference point. The learning rate decreases linearly after a warmup period.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use the Hugging Face optimizer \n",
    "from transformers import AdamW\n",
    "\n",
    "# specify the optimizer \n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',return_dict=True)\n",
    "optimizer = AdamW(model.parameters(), lr =  3e-5)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 100, num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the top layers\n",
    "#for param in model.base_model.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  total_loss = 0.0\n",
    "  total_preds=[]\n",
    "\n",
    "  # Set model to training mode\n",
    "  model.train()  \n",
    "  #model.zero_grad()        \n",
    "\n",
    "  # Iterate over the batch in dataloader\n",
    "  for step,batch in enumerate(train_dataloader):  \n",
    "    # Get it batch to leverage device\n",
    "    batch = [r.to(device) for r in batch]\n",
    "    input_ids, mask, labels = batch\n",
    "\n",
    "    model.zero_grad() \n",
    "    outputs = model(input_ids,attention_mask=mask, labels=labels)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits \n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss\n",
    "\n",
    "    # backward pass \n",
    "    loss.backward()\n",
    "\n",
    "    # Reduce the effects of the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate.\n",
    "    scheduler.step()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(outputs)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  total_loss = 0.0\n",
    "  total_preds = []\n",
    "\n",
    "  # Set model to evaluation mode\n",
    "  model.eval()   \n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    batch = [t.to(device) for t in batch]\n",
    "    input_ids, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      #outputs = model(input_ids, labels=labels)\n",
    "      outputs = model(input_ids,attention_mask=mask, labels=labels)\n",
    "\n",
    "      loss = outputs.loss\n",
    "      logits = outputs.logits\n",
    "        \n",
    "      # add on to the total loss\n",
    "      total_loss = total_loss + loss\n",
    "      total_preds.append(outputs)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "source": [
    "### Fine-tuning the RoBERTa model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      " Loss: 0.617 - Val_Loss: 0.576\n",
      "\n",
      " Epoch 2 / 2\n",
      " Loss: 0.511 - Val_Loss: 0.565\n"
     ]
    }
   ],
   "source": [
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "#push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Specify the name of the saved weights file\n",
    "saved_file = \"fakenewsnlp-saved_weights.pt\"\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(num_epocs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, num_epocs))\n",
    "    \n",
    "    train_loss = train()\n",
    "    val_loss = evaluate()\n",
    "    \n",
    "    print(f' Loss: {train_loss:.3f} - Val_Loss: {val_loss:.3f}')\n",
    "\n",
    "    #save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(),  saved_file)\n",
    "    \n",
    "    # Track the training/validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(val_loss) \n",
    "\n",
    "# Release the memory in GPU\n",
    "model.cpu()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "source": [
    "## Testing the Fine-tuned Model\n",
    "Let's run the fine-tuned RoBERTa model on the test dataset. \n",
    "Similar to how we prepared the training and validation datasets earlier, we will start by tokenizing the test data, performing truncation, and padding.\n",
    "\n",
    "We iterate through multiple batches of data provided by _test_dataloader_. \n",
    "To obtain the predicted label, we use _torch.argmax()_ to get the label using the logits that are provided. \n",
    "The predicted results are then stored in the variable _predictions_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer(\n",
    "    X_test.to_list(),\n",
    "    padding='max_length',\n",
    "    max_length = max_length,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input_ids, attention_mask and labels to tensor\n",
    "test_input_ids_tensor = torch.tensor(tokenized_test[\"input_ids\"])\n",
    "test_attention_mask_tensor = torch.tensor(tokenized_test[\"attention_mask\"])\n",
    "test_labels_tensor = torch.tensor(y_test.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tensor = TensorDataset(test_input_ids_tensor,\n",
    "                               test_attention_mask_tensor, \n",
    "                               test_labels_tensor)\n",
    "\n",
    "test_dataloader = DataLoader(test_data_tensor, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the model to evaluation mode\n",
    "model.eval()\n",
    "predictions, true_labels = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "total_preds = []\n",
    "predictions = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()   \n",
    "  \n",
    "# iterate over batches\n",
    "for step,batch in enumerate(test_dataloader):\n",
    "   batch = [t.to(device) for t in batch]\n",
    "   input_ids, mask, labels = batch\n",
    "\n",
    "   # deactivate autograd\n",
    "   with torch.no_grad():\n",
    "      outputs = model(input_ids,attention_mask=mask)\n",
    "      logits = outputs.logits\n",
    "    \n",
    "      predictions.append(torch.argmax(logits, dim=1).tolist())\n",
    "      total_preds.append(outputs)\n",
    "\n",
    "\n",
    "model.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_f = list(np.concatenate(predictions).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "ground_truth = y_test.tolist()\n",
    "ground_truth[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.76      0.52      0.62       816\n           1       0.66      0.85      0.74       885\n\n    accuracy                           0.69      1701\n   macro avg       0.71      0.69      0.68      1701\nweighted avg       0.71      0.69      0.68      1701\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = y_test.tolist()\n",
    "y_pred =  list(np.concatenate(predictions).flat)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[426 390]\n [131 754]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print(confusion_matrix(ground_truth, predictions_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.6937\nF1 score: 0.7432\nAUC: 0.6870\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy_score(ground_truth,predictions_f))\n",
    "print(\"F1 score: %.4f\" % f1_score(ground_truth,predictions_f))\n",
    "print(\"AUC: %.4f\" % roc_auc_score(ground_truth,predictions_f))"
   ]
  }
 ]
}